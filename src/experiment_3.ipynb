{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# G-Mixup can improve the performance of graph neural networks on various datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import os.path as osp\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from gmixup import prepare_dataset_onehot_y\n",
    "from utils import stat_graph\n",
    "import numpy as np\n",
    "from src.graphon_estimator import largest_gap\n",
    "from src.utils import split_class_graphs, align_graphs\n",
    "from torch_geometric.loader import DataLoader\n",
    "from src.gmixup import prepare_dataset_x\n",
    "from src.utils import two_graphons_mixup\n",
    "from models import GIN, GCN, DiffPoolNet, TopKNet, MinCutPoolNet\n",
    "from src.gmixup import mixup_cross_entropy_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './'\n",
    "dataset_names = ['IMDB-MULTI', 'IMDB-BINARY', 'REDDIT-BINARY', 'REDDIT-MULTI-5K', 'REDDIT-MULTI-12K']\n",
    "models = ['GCN', 'GIN', 'MinCutPool', 'DiffPool', 'TopKPool']\n",
    "epochs = 300\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "num_hidden = 64\n",
    "seeds = [1314, 311098, 271296, 180562, 280466, 50832, 280433, 21022, 0, 546464]\n",
    "no_test_runs = 10\n",
    "lam_range = [0.1, 0.2]\n",
    "aug_ratio = 0.2\n",
    "aug_num = 10\n",
    "augmentations = ['G-Mixup', 'Vanilla']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Running device: {device}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    graph_all = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in train_loader:\n",
    "        # print( \"data.y\", data.y )\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        y = data.y.view(-1, num_classes)\n",
    "        #print(y.size())\n",
    "        #print(output.size())\n",
    "        loss = mixup_cross_entropy_loss(output, y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        graph_all += data.num_graphs\n",
    "        optimizer.step()\n",
    "        y = y.max(dim=1)[1]\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        total += data.num_graphs\n",
    "\n",
    "    loss = loss_all / graph_all\n",
    "    acc = correct / total\n",
    "    return model, loss, acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        y = data.y.view(-1, num_classes)\n",
    "        loss += mixup_cross_entropy_loss(output, y).item() * data.num_graphs\n",
    "        y = y.max(dim=1)[1]\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        total += data.num_graphs\n",
    "    acc = correct / total\n",
    "    loss = loss / total\n",
    "    return acc, loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg num nodes of training graphs: 13.243809523809524\n",
      "Avg num edges of training graphs: 68.67238095238095\n",
      "Avg density of training graphs: 0.39152207454671556\n",
      "Median num edges of training graphs: 62.0\n",
      "Median density of training graphs: 0.62\n",
      "Avg num nodes of new training graphs: 12.369047619047619\n",
      "Avg num edges of new training graphs: 59.18412698412698\n",
      "Avg density of new training graphs: 0.38684120086593965\n",
      "Median num edges of new training graphs: 62.0\n",
      "Median density of new training graphs: 0.62\n",
      "Epoch: 001, Train Loss: 10.362714, Val Loss: 2.915598, Test Loss: 2.735078, Train acc:  0.379365, Val Acc:  0.426667, Test Acc:  0.436667\n",
      "Epoch: 002, Train Loss: 1.865427, Val Loss: 1.472195, Test Loss: 1.355790, Train acc:  0.396032, Val Acc:  0.386667, Test Acc:  0.413333\n",
      "Epoch: 003, Train Loss: 1.220684, Val Loss: 1.469202, Test Loss: 1.262885, Train acc:  0.448413, Val Acc:  0.453333, Test Acc:  0.420000\n",
      "Epoch: 004, Train Loss: 1.098100, Val Loss: 1.171122, Test Loss: 1.130336, Train acc:  0.464286, Val Acc:  0.460000, Test Acc:  0.433333\n",
      "Epoch: 005, Train Loss: 1.041612, Val Loss: 1.030834, Test Loss: 1.051327, Train acc:  0.488889, Val Acc:  0.480000, Test Acc:  0.473333\n",
      "Epoch: 006, Train Loss: 1.001498, Val Loss: 1.380160, Test Loss: 1.030719, Train acc:  0.490476, Val Acc:  0.466667, Test Acc:  0.466667\n",
      "Epoch: 007, Train Loss: 1.007495, Val Loss: 1.030911, Test Loss: 0.990429, Train acc:  0.511111, Val Acc:  0.446667, Test Acc:  0.513333\n",
      "Epoch: 008, Train Loss: 0.989664, Val Loss: 1.158656, Test Loss: 1.007911, Train acc:  0.523016, Val Acc:  0.466667, Test Acc:  0.526667\n",
      "Epoch: 009, Train Loss: 0.986142, Val Loss: 1.103970, Test Loss: 0.964647, Train acc:  0.519048, Val Acc:  0.466667, Test Acc:  0.510000\n",
      "Epoch: 010, Train Loss: 0.971082, Val Loss: 0.987331, Test Loss: 1.046094, Train acc:  0.526984, Val Acc:  0.473333, Test Acc:  0.496667\n",
      "Epoch: 011, Train Loss: 1.006234, Val Loss: 1.319916, Test Loss: 1.029485, Train acc:  0.515079, Val Acc:  0.473333, Test Acc:  0.513333\n",
      "Epoch: 012, Train Loss: 0.972019, Val Loss: 1.025678, Test Loss: 0.996321, Train acc:  0.533333, Val Acc:  0.473333, Test Acc:  0.496667\n",
      "Epoch: 013, Train Loss: 0.966758, Val Loss: 1.033457, Test Loss: 0.968886, Train acc:  0.526984, Val Acc:  0.500000, Test Acc:  0.476667\n",
      "Epoch: 014, Train Loss: 0.957857, Val Loss: 0.991151, Test Loss: 0.954427, Train acc:  0.534921, Val Acc:  0.446667, Test Acc:  0.533333\n",
      "Epoch: 015, Train Loss: 0.964790, Val Loss: 0.992128, Test Loss: 0.940175, Train acc:  0.531746, Val Acc:  0.486667, Test Acc:  0.490000\n",
      "Epoch: 016, Train Loss: 0.954281, Val Loss: 1.018139, Test Loss: 1.003038, Train acc:  0.534127, Val Acc:  0.473333, Test Acc:  0.503333\n",
      "Epoch: 017, Train Loss: 0.981283, Val Loss: 1.112260, Test Loss: 0.979149, Train acc:  0.526984, Val Acc:  0.460000, Test Acc:  0.490000\n",
      "Epoch: 018, Train Loss: 0.978345, Val Loss: 0.988817, Test Loss: 1.013484, Train acc:  0.522222, Val Acc:  0.486667, Test Acc:  0.456667\n",
      "Epoch: 019, Train Loss: 0.959969, Val Loss: 1.020490, Test Loss: 0.958927, Train acc:  0.534127, Val Acc:  0.453333, Test Acc:  0.506667\n",
      "Epoch: 020, Train Loss: 0.944546, Val Loss: 0.987707, Test Loss: 0.947846, Train acc:  0.545238, Val Acc:  0.473333, Test Acc:  0.490000\n",
      "Epoch: 021, Train Loss: 0.943742, Val Loss: 0.985431, Test Loss: 0.949423, Train acc:  0.534921, Val Acc:  0.486667, Test Acc:  0.510000\n",
      "Epoch: 022, Train Loss: 0.954959, Val Loss: 1.042023, Test Loss: 0.950743, Train acc:  0.530159, Val Acc:  0.480000, Test Acc:  0.473333\n",
      "Epoch: 023, Train Loss: 0.942449, Val Loss: 0.985219, Test Loss: 0.932954, Train acc:  0.541270, Val Acc:  0.473333, Test Acc:  0.510000\n",
      "Epoch: 024, Train Loss: 0.940602, Val Loss: 1.005013, Test Loss: 0.949087, Train acc:  0.542857, Val Acc:  0.433333, Test Acc:  0.523333\n",
      "Epoch: 025, Train Loss: 0.934189, Val Loss: 0.981167, Test Loss: 0.941098, Train acc:  0.542857, Val Acc:  0.460000, Test Acc:  0.526667\n",
      "Epoch: 026, Train Loss: 0.937477, Val Loss: 0.988749, Test Loss: 0.951106, Train acc:  0.547619, Val Acc:  0.486667, Test Acc:  0.486667\n",
      "Epoch: 027, Train Loss: 0.940959, Val Loss: 0.982738, Test Loss: 0.947145, Train acc:  0.536508, Val Acc:  0.466667, Test Acc:  0.513333\n",
      "Epoch: 028, Train Loss: 0.932688, Val Loss: 1.062601, Test Loss: 0.964485, Train acc:  0.544444, Val Acc:  0.453333, Test Acc:  0.510000\n",
      "Epoch: 029, Train Loss: 0.938479, Val Loss: 0.992670, Test Loss: 0.968799, Train acc:  0.547619, Val Acc:  0.486667, Test Acc:  0.476667\n",
      "Epoch: 030, Train Loss: 0.942974, Val Loss: 1.012845, Test Loss: 0.970377, Train acc:  0.546825, Val Acc:  0.466667, Test Acc:  0.513333\n",
      "Epoch: 031, Train Loss: 0.943760, Val Loss: 0.980872, Test Loss: 0.958791, Train acc:  0.551587, Val Acc:  0.466667, Test Acc:  0.496667\n",
      "Epoch: 032, Train Loss: 0.931745, Val Loss: 0.982561, Test Loss: 0.940150, Train acc:  0.549206, Val Acc:  0.500000, Test Acc:  0.536667\n",
      "Epoch: 033, Train Loss: 0.927456, Val Loss: 0.992391, Test Loss: 0.940102, Train acc:  0.542857, Val Acc:  0.500000, Test Acc:  0.523333\n",
      "Epoch: 034, Train Loss: 0.936413, Val Loss: 0.966008, Test Loss: 0.940161, Train acc:  0.547619, Val Acc:  0.473333, Test Acc:  0.520000\n",
      "Epoch: 035, Train Loss: 0.937358, Val Loss: 1.021879, Test Loss: 0.944109, Train acc:  0.538095, Val Acc:  0.486667, Test Acc:  0.526667\n",
      "Epoch: 036, Train Loss: 0.932495, Val Loss: 0.977419, Test Loss: 0.941822, Train acc:  0.556349, Val Acc:  0.500000, Test Acc:  0.526667\n",
      "Epoch: 037, Train Loss: 0.921093, Val Loss: 0.982123, Test Loss: 0.939912, Train acc:  0.550000, Val Acc:  0.466667, Test Acc:  0.523333\n",
      "Epoch: 038, Train Loss: 0.926620, Val Loss: 0.960653, Test Loss: 0.938483, Train acc:  0.550794, Val Acc:  0.486667, Test Acc:  0.526667\n",
      "Epoch: 039, Train Loss: 0.925858, Val Loss: 1.018300, Test Loss: 0.938580, Train acc:  0.553968, Val Acc:  0.480000, Test Acc:  0.520000\n",
      "Epoch: 040, Train Loss: 0.930696, Val Loss: 0.970865, Test Loss: 1.009685, Train acc:  0.545238, Val Acc:  0.473333, Test Acc:  0.523333\n",
      "Epoch: 041, Train Loss: 0.959882, Val Loss: 0.988111, Test Loss: 0.938478, Train acc:  0.545238, Val Acc:  0.466667, Test Acc:  0.530000\n",
      "Epoch: 042, Train Loss: 0.940734, Val Loss: 0.971589, Test Loss: 1.065062, Train acc:  0.555556, Val Acc:  0.520000, Test Acc:  0.510000\n",
      "Epoch: 043, Train Loss: 0.941190, Val Loss: 1.113453, Test Loss: 1.020909, Train acc:  0.532540, Val Acc:  0.453333, Test Acc:  0.530000\n",
      "Epoch: 044, Train Loss: 0.951541, Val Loss: 0.948729, Test Loss: 1.087095, Train acc:  0.555556, Val Acc:  0.486667, Test Acc:  0.483333\n",
      "Epoch: 045, Train Loss: 0.936809, Val Loss: 1.026698, Test Loss: 0.959914, Train acc:  0.553175, Val Acc:  0.486667, Test Acc:  0.530000\n",
      "Epoch: 046, Train Loss: 0.931823, Val Loss: 1.026699, Test Loss: 1.013418, Train acc:  0.557937, Val Acc:  0.466667, Test Acc:  0.520000\n",
      "Epoch: 047, Train Loss: 0.928146, Val Loss: 0.976667, Test Loss: 0.968659, Train acc:  0.550000, Val Acc:  0.486667, Test Acc:  0.513333\n",
      "Epoch: 048, Train Loss: 0.917423, Val Loss: 0.997606, Test Loss: 1.012435, Train acc:  0.564286, Val Acc:  0.446667, Test Acc:  0.500000\n",
      "Epoch: 049, Train Loss: 0.926559, Val Loss: 0.982243, Test Loss: 0.992202, Train acc:  0.552381, Val Acc:  0.493333, Test Acc:  0.500000\n",
      "Epoch: 050, Train Loss: 0.939852, Val Loss: 0.954209, Test Loss: 0.945431, Train acc:  0.545238, Val Acc:  0.506667, Test Acc:  0.543333\n",
      "Epoch: 051, Train Loss: 1.005278, Val Loss: 1.781130, Test Loss: 2.107161, Train acc:  0.543651, Val Acc:  0.420000, Test Acc:  0.433333\n",
      "Epoch: 052, Train Loss: 1.259732, Val Loss: 1.338172, Test Loss: 1.332150, Train acc:  0.499206, Val Acc:  0.426667, Test Acc:  0.443333\n",
      "Epoch: 053, Train Loss: 1.224763, Val Loss: 1.158552, Test Loss: 1.143129, Train acc:  0.488889, Val Acc:  0.433333, Test Acc:  0.510000\n",
      "Epoch: 054, Train Loss: 1.064006, Val Loss: 1.428654, Test Loss: 1.441056, Train acc:  0.518254, Val Acc:  0.440000, Test Acc:  0.386667\n",
      "Epoch: 055, Train Loss: 1.152431, Val Loss: 1.249108, Test Loss: 1.152352, Train acc:  0.503968, Val Acc:  0.473333, Test Acc:  0.396667\n",
      "Epoch: 056, Train Loss: 1.156655, Val Loss: 1.237704, Test Loss: 1.019500, Train acc:  0.483333, Val Acc:  0.406667, Test Acc:  0.510000\n",
      "Epoch: 057, Train Loss: 1.000816, Val Loss: 1.048471, Test Loss: 0.952808, Train acc:  0.485714, Val Acc:  0.466667, Test Acc:  0.536667\n",
      "Epoch: 058, Train Loss: 0.950756, Val Loss: 1.023067, Test Loss: 0.978514, Train acc:  0.557937, Val Acc:  0.460000, Test Acc:  0.533333\n",
      "Epoch: 059, Train Loss: 0.934644, Val Loss: 1.073655, Test Loss: 0.931641, Train acc:  0.542063, Val Acc:  0.433333, Test Acc:  0.523333\n",
      "Epoch: 060, Train Loss: 0.937502, Val Loss: 1.109697, Test Loss: 0.963048, Train acc:  0.538889, Val Acc:  0.500000, Test Acc:  0.533333\n",
      "Epoch: 061, Train Loss: 0.938242, Val Loss: 1.082078, Test Loss: 0.944781, Train acc:  0.548413, Val Acc:  0.480000, Test Acc:  0.560000\n",
      "Epoch: 062, Train Loss: 0.919921, Val Loss: 1.015558, Test Loss: 0.944295, Train acc:  0.560317, Val Acc:  0.473333, Test Acc:  0.530000\n",
      "Epoch: 063, Train Loss: 0.924549, Val Loss: 1.061602, Test Loss: 0.930040, Train acc:  0.565079, Val Acc:  0.453333, Test Acc:  0.556667\n",
      "Epoch: 064, Train Loss: 0.918098, Val Loss: 1.029573, Test Loss: 0.927009, Train acc:  0.569048, Val Acc:  0.440000, Test Acc:  0.546667\n",
      "Epoch: 065, Train Loss: 0.911905, Val Loss: 1.031451, Test Loss: 0.928456, Train acc:  0.565873, Val Acc:  0.466667, Test Acc:  0.543333\n",
      "Epoch: 066, Train Loss: 0.912834, Val Loss: 1.019304, Test Loss: 0.925236, Train acc:  0.562698, Val Acc:  0.480000, Test Acc:  0.543333\n",
      "Epoch: 067, Train Loss: 0.910036, Val Loss: 1.028843, Test Loss: 0.932123, Train acc:  0.560317, Val Acc:  0.453333, Test Acc:  0.550000\n",
      "Epoch: 068, Train Loss: 0.911139, Val Loss: 1.051391, Test Loss: 0.926676, Train acc:  0.565079, Val Acc:  0.480000, Test Acc:  0.556667\n",
      "Epoch: 069, Train Loss: 0.913836, Val Loss: 1.007707, Test Loss: 0.924162, Train acc:  0.564286, Val Acc:  0.460000, Test Acc:  0.536667\n",
      "Epoch: 070, Train Loss: 0.908423, Val Loss: 1.044864, Test Loss: 0.933149, Train acc:  0.564286, Val Acc:  0.426667, Test Acc:  0.546667\n",
      "Epoch: 071, Train Loss: 0.908067, Val Loss: 1.025603, Test Loss: 0.914854, Train acc:  0.568254, Val Acc:  0.473333, Test Acc:  0.563333\n",
      "Epoch: 072, Train Loss: 0.906531, Val Loss: 1.032751, Test Loss: 0.924912, Train acc:  0.564286, Val Acc:  0.440000, Test Acc:  0.543333\n",
      "Epoch: 073, Train Loss: 0.906244, Val Loss: 1.033858, Test Loss: 0.931800, Train acc:  0.575397, Val Acc:  0.426667, Test Acc:  0.553333\n",
      "Epoch: 074, Train Loss: 0.901791, Val Loss: 1.023786, Test Loss: 0.923604, Train acc:  0.571429, Val Acc:  0.460000, Test Acc:  0.546667\n",
      "Epoch: 075, Train Loss: 0.903653, Val Loss: 0.999693, Test Loss: 0.923823, Train acc:  0.567460, Val Acc:  0.433333, Test Acc:  0.546667\n",
      "Epoch: 076, Train Loss: 0.906995, Val Loss: 1.034101, Test Loss: 0.934016, Train acc:  0.572222, Val Acc:  0.466667, Test Acc:  0.553333\n",
      "Epoch: 077, Train Loss: 0.898358, Val Loss: 1.031166, Test Loss: 0.925184, Train acc:  0.574603, Val Acc:  0.440000, Test Acc:  0.550000\n",
      "Epoch: 078, Train Loss: 0.897963, Val Loss: 1.018293, Test Loss: 0.930708, Train acc:  0.564286, Val Acc:  0.473333, Test Acc:  0.536667\n",
      "Epoch: 079, Train Loss: 0.901674, Val Loss: 1.023848, Test Loss: 0.935824, Train acc:  0.564286, Val Acc:  0.466667, Test Acc:  0.533333\n",
      "Epoch: 080, Train Loss: 0.898359, Val Loss: 1.045437, Test Loss: 0.924232, Train acc:  0.573810, Val Acc:  0.473333, Test Acc:  0.540000\n",
      "Epoch: 081, Train Loss: 0.895859, Val Loss: 1.008737, Test Loss: 0.919601, Train acc:  0.576190, Val Acc:  0.460000, Test Acc:  0.546667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     94\u001B[0m best_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, epochs):\n\u001B[1;32m---> 96\u001B[0m     model, train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     97\u001B[0m     val_acc, val_loss \u001B[38;5;241m=\u001B[39m test(model, val_loader)\n\u001B[0;32m     98\u001B[0m     test_acc, test_loss \u001B[38;5;241m=\u001B[39m test(model, test_loader)\n",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_loader)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m#print(y.size())\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m#print(output.size())\u001B[39;00m\n\u001B[0;32m     15\u001B[0m loss \u001B[38;5;241m=\u001B[39m mixup_cross_entropy_loss(output, y)\n\u001B[1;32m---> 16\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m loss_all \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m data\u001B[38;5;241m.\u001B[39mnum_graphs\n\u001B[0;32m     18\u001B[0m graph_all \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mnum_graphs\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:363\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    355\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    356\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    357\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    361\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    362\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 363\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    path = osp.join(data_path, dataset_name)\n",
    "    dataset = TUDataset(path, name=dataset_name)\n",
    "    dataset = list(dataset)\n",
    "    for graph in dataset:\n",
    "        graph.y = graph.y.view(-1)\n",
    "\n",
    "    dataset = prepare_dataset_onehot_y(dataset)\n",
    "    train_nums = int(len(dataset) * 0.7)\n",
    "    train_val_nums = int(len(dataset) * 0.8)\n",
    "\n",
    "    avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(dataset[:train_nums])\n",
    "    graphon_size = int(median_num_nodes)\n",
    "    print(f\"Avg num nodes of training graphs: {avg_num_nodes}\")\n",
    "    print(f\"Avg num edges of training graphs: {avg_num_edges}\")\n",
    "    print(f\"Avg density of training graphs: {avg_density}\")\n",
    "    print(f\"Median num edges of training graphs: {median_num_edges}\")\n",
    "    print(f\"Median density of training graphs: {median_density}\")\n",
    "    for model_name in models:\n",
    "        for seed in seeds:\n",
    "            torch.manual_seed(seed)\n",
    "            random.seed(seed)\n",
    "            random.shuffle(dataset)\n",
    "            for aug in augmentations:\n",
    "                random.shuffle(dataset)\n",
    "                if aug == 'G-Mixup':\n",
    "                    class_graphs = split_class_graphs(dataset[:train_nums])\n",
    "                    graphons = []\n",
    "                    for label, graphs in class_graphs:\n",
    "                        align_graphs_list, normalized_node_degrees, max_num, min_num = align_graphs(\n",
    "                            graphs, padding=True, N=graphon_size)\n",
    "                        graphon = largest_gap(align_graphs_list, k=graphon_size)\n",
    "                        graphons.append((label, graphon))\n",
    "\n",
    "                    num_sample = int(train_nums * aug_ratio / aug_num)\n",
    "                    lam_list = np.random.uniform(low=lam_range[0], high=lam_range[1], size=(aug_num,))\n",
    "\n",
    "                    random.seed(seed)\n",
    "                    new_graph = []\n",
    "                    for lam in lam_list:\n",
    "                        two_graphons = random.sample(graphons, 2)\n",
    "                        new_graph += two_graphons_mixup(two_graphons, la=lam, num_sample=num_sample)\n",
    "\n",
    "                    new_dataset = new_graph + dataset\n",
    "                    new_train_nums = train_nums + len(new_graph)\n",
    "                    new_train_val_nums = train_val_nums + len(new_graph)\n",
    "                else:\n",
    "                    new_dataset = dataset\n",
    "                    new_train_nums = train_nums\n",
    "                    new_train_val_nums = train_val_nums\n",
    "\n",
    "                dataset = prepare_dataset_x(new_dataset)\n",
    "\n",
    "                num_features = new_dataset[0].x.shape[1]\n",
    "                num_classes = new_dataset[0].y.shape[0]\n",
    "\n",
    "                # avg_num_nodes, avg_num_edges, avg_density, median_num_nodes, median_num_edges, median_density = stat_graph(new_dataset[:new_train_nums])\n",
    "                # print(f\"Avg num nodes of new training graphs: {avg_num_nodes}\")\n",
    "                # print(f\"Avg num edges of new training graphs: {avg_num_edges}\")\n",
    "                # print(f\"Avg density of new training graphs: {avg_density}\")\n",
    "                # print(f\"Median num edges of new training graphs: {median_num_edges}\")\n",
    "                # print(f\"Median density of new training graphs: {median_density}\")\n",
    "                train_dataset = new_dataset[:new_train_nums]\n",
    "                random.shuffle(train_dataset)\n",
    "                val_dataset = new_dataset[new_train_nums:new_train_val_nums]\n",
    "                test_dataset = new_dataset[new_train_val_nums:]\n",
    "\n",
    "                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "                if model_name == \"GIN\":\n",
    "                    model = GIN(num_features=num_features, num_classes=num_classes, num_hidden=num_hidden).to(device)\n",
    "                elif model_name == \"GCN\":\n",
    "                    model = GCN(in_channels=num_features, hidden_channels=num_hidden, out_channels=num_classes, num_layers=4).to(device)\n",
    "                elif model_name == \"TopKPool\":\n",
    "                    model = TopKNet(in_channels=num_features, hidden_channels=num_hidden, out_channels=num_classes).to(device)\n",
    "                elif model_name == \"DiffPool\":\n",
    "                    model = DiffPoolNet(in_channels=num_features, hidden_channels=num_hidden, out_channels=num_classes, max_nodes = median_num_nodes).to(device)\n",
    "                elif model_name == \"MinCutPool\":\n",
    "                    model = MinCutPoolNet(in_channels=num_features, hidden_channels=num_hidden, out_channels=num_classes, max_nodes = median_num_nodes).to(device)\n",
    "                else:\n",
    "                    model = None\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "                scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "                max_val_acc = 0\n",
    "                model_test_acc = 0\n",
    "                model_test_loss = 0\n",
    "                model_val_loss = 0\n",
    "                best_epoch = 0\n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                test_losses = []\n",
    "                for epoch in range(1, epochs):\n",
    "                    model, train_loss, train_acc = train(model, train_loader)\n",
    "                    val_acc, val_loss = test(model, val_loader)\n",
    "                    test_acc, test_loss = test(model, test_loader)\n",
    "                    scheduler.step()\n",
    "                    train_losses.append(train_loss)\n",
    "                    val_losses.append(val_loss)\n",
    "                    test_losses.append(test_loss)\n",
    "                    if val_acc > max_val_acc:\n",
    "                        max_val_acc = val_acc\n",
    "                        model_test_loss = test_loss\n",
    "                        model_test_acc = test_acc\n",
    "                        model_val_loss = model_val_loss\n",
    "                        best_epoch = epoch\n",
    "\n",
    "                    print(\n",
    "                        'Epoch: {:03d}, Train Loss: {:.6f}, Val Loss: {:.6f}, Test Loss: {:.6f}, Train acc: {: .6f}, Val Acc: {: .6f}, Test Acc: {: .6f}'.format(\n",
    "                            epoch, train_loss, val_loss, test_loss, train_acc, val_acc, test_acc))\n",
    "\n",
    "                with open('train_log.txt', 'a') as f:\n",
    "                    f.write(f'Dataset: {dataset_name}, Model: {model_name}, Seed: {seed}, Aug: {aug}, Best epoch: {best_epoch}, Test acc: {model_test_acc}, Test loss: {model_test_loss}, Val acc: {max_val_acc}, Val loss: {model_val_loss}\\n')\n",
    "                if model_name == 'GCN':\n",
    "                    with open('./results/losses.txt', 'a') as f:\n",
    "                        f.write(f'{dataset_name}, {seed}, train, {train_losses}\\n{dataset_name}, {seed}, val, {val_losses}\\n{dataset_name}, {seed}, test, {test_losses}\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
